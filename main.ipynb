{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormality detection of danish address valuations\n",
    "**02807 Computational tools for data science**\n",
    "\n",
    "All code can be found in our GitHub repository: [https://github.com/PBonvang/02807-Project](https://github.com/PBonvang/02807-Project)\n",
    "\n",
    "Data should now be available at [https://drive.proton.me/urls/89G58EHFV4#8H7CXNedYN7k](https://drive.proton.me/urls/89G58EHFV4#8H7CXNedYN7k). \n",
    "\n",
    "You may contact s214606@dtu.dk if the link is locked or s214658@student.dtu.dk if GitHub repository is unavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and imports\n",
    "from pathlib import Path\n",
    "import sys, os, glob, json, csv, re, pickle, warnings\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, concat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from requests_ratelimiter import LimiterSession\n",
    "\n",
    "from analysis_config import PREDICTION_VAR\n",
    "from config import LIST_IDENTIFIER,BBR_DATA_PATH, DATA_FOLDER\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most cells in this notebook refer to files in the GitHub repo e.g \n",
    "```py\n",
    "# utils/preprocessing\n",
    "```\n",
    "represents our preprocessing functions file in utils directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "DATA_FOLDER = Path('Data')\n",
    "\n",
    "BBR_DATA_FILENAME = 'BBR_FULL.json'\n",
    "BBR_DATA_PATH = DATA_FOLDER / BBR_DATA_FILENAME\n",
    "LIST_IDENTIFIER = 'List\":'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_config\n",
    "DATA_FOLDER = 'data'\n",
    "PREDICTION_VAR = 'totalAddressValue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data\n",
    "Reminder that BBR and DAR are either downloaded from datafordeleren.dk or their FTP server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping property valuations (\"VUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape . Run from command line\n",
    "session = LimiterSession(per_second=10)\n",
    "\n",
    "def property_assessment(addressID: str, verbose=False):\n",
    "    url = \"https://api-fs.vurderingsportalen.dk/preliminaryproperties/_search\"\n",
    "\n",
    "    query = {\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"must\": [\n",
    "            {\n",
    "              \"match_phrase\": {\n",
    "                \"adgangsAdresseID\": f\"{addressID}\"\n",
    "              }\n",
    "            }\n",
    "          ],\n",
    "          \"filter\": [\n",
    "            {\n",
    "              \"bool\": {\n",
    "                \"should\": []\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"match\": {\n",
    "                \"documentType\": 4\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    \n",
    "    response = session.post(url, json=query, headers=headers)\n",
    "\n",
    "    if verbose:\n",
    "        if response.status_code == 200:\n",
    "            print(\"Success:\")\n",
    "            print(response.json())\n",
    "        else:\n",
    "            print(\"Failed to fetch data:\")\n",
    "            print(\"Status code:\", response.status_code)\n",
    "            print(\"Response:\", response.text)\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def scrape(idx_start:int, idx_end:int, DAR_PATH:str='DAR_joined.csv', debug=False):\n",
    "    \"\"\"\n",
    "    Can be run with \"python <path to scrape.py> <idx_start> <idx_end> <DAR_PATH> <debug>\"\n",
    "    \n",
    "    Args:\n",
    "        idx_start (int): Start index of scraping. Must be greater than or equal to 0\n",
    "        idx_end (int): End index of scraping. Must be less or equal to 2259957\n",
    "        DAR_PATH (str): Path to \"DAR_joined.csv\"\n",
    "        debug (bool, optional): Enables verbose and checks if 3 debug samples or scraped (index 0 to 2). Defaults to False.\n",
    "    \"\"\"\n",
    "    columns = {'id':int,'adgangsAdresseID':str,\n",
    "               'vurderingsaar':int,'propertyValue':float,\n",
    "               'groundValue':float,'propertyTax':float,\n",
    "               'groundTax':float,'totalAddressTax':float}\n",
    "\n",
    "    debug_samples = ['0a3f507c-036b-32b8-e044-0003ba298018','2edda757-9218-1605-e044-0003ba298018','0a3f5083-e355-32b8-e044-0003ba298018'] #Middle one is invalid\n",
    "\n",
    "    output_file = f'VUR_{idx_start}_to_{idx_end}.csv'\n",
    "    \n",
    "    pd.DataFrame(columns=columns.keys()).to_csv(output_file, index=False, sep=';')\n",
    "\n",
    "    if debug:\n",
    "        addressIDs = debug_samples\n",
    "    else:\n",
    "        addressIDs = pd.read_csv(DAR_PATH, delimiter=';').husnummer_id.iloc[idx_start:idx_end]\n",
    "\n",
    "    for id in tqdm(addressIDs, desc='Scraping in progress', total=len(addressIDs)):\n",
    "        response = property_assessment(id, verbose=debug)\n",
    "        try:\n",
    "            source = response['hits']['hits'][0]['_source']\n",
    "            taxCalculations = source.pop('taxCalculation')\n",
    "            source.update({k:re.sub(\"[^0-9]\", \"\", v) for k, v in taxCalculations.items()})\n",
    "            pd.DataFrame({key:source[key] for key in columns.keys()},index=[0]).to_csv(output_file, mode='a',index=False, header=False, sep=';')\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        except KeyError as error:\n",
    "            \n",
    "            if error.args[0] == 'taxCalculation': # Basically fill tax columns with NaNs\n",
    "              for k in columns.keys():\n",
    "                pattern = re.compile(r'tax', re.IGNORECASE)\n",
    "                if pattern.search(k):\n",
    "                  source[k] = np.nan\n",
    "            pd.DataFrame({key:source[key] for key in columns.keys()},index=[0]).to_csv(output_file, mode='a',index=False, header=False, sep=';')\n",
    "                  \n",
    "        \n",
    "    VUR_edit = pd.read_csv(output_file, delimiter=';')\n",
    "    for col, dtype in columns.items():\n",
    "        VUR_edit[col] = VUR_edit[col].astype(dtype)\n",
    "    VUR_edit.to_csv(output_file,index=False, header=True, sep=';')\n",
    "\n",
    "scrape(*map(int,sys.argv[1:3]),*sys.argv[3:5]) # Run it CL (easier like this for HPC and other machines) e.g scrape(1123328,1123332,r'<path_to>\\DAR_joined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was run on HPC with\n",
    "```bash\n",
    "data/submit.sh\n",
    "```\n",
    "partition example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#BSUB -J VUR\n",
    "#BSUB -o VUR_LOG_%J.out  \n",
    "#BSUB -q hpc\n",
    "#BSUB -W 70:00\n",
    "#BSUB -R \"rusage[mem=8GB]\"\n",
    "#BSUB -n 12\n",
    "#BSUB -R \"span[hosts=1]\"\n",
    "#BSUB -N\n",
    "\n",
    "set -e\n",
    "module load python3/3.9.11\n",
    "source ../../.venv/bin/activate\n",
    "python3 -m pip install -r ../../requirements.txt\n",
    "python3 scrape.py 2000000 2259957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/preprocessing\n",
    "\n",
    "from config import LIST_IDENTIFIER\n",
    "\n",
    "def get_attribute_from_json_line(line: str)->str:\n",
    "    return line.split(':')[0][1:-1]\n",
    "\n",
    "def clean_file(data_path: Path, output_path: Path) -> None:\n",
    "    with tqdm(total=data_path.stat().st_size) as pbar:\n",
    "        with data_path.open(encoding='utf-8') as data_file:\n",
    "            with output_path.open('w', encoding='utf-8') as output_file:\n",
    "                while line := data_file.readline():\n",
    "                    if line.strip() == '': continue\n",
    "\n",
    "                    output_file.write(line)\n",
    "\n",
    "def get_file_metadata(data_path: Path, save_path: Path = None)->dict:\n",
    "    file_metadata = dict()\n",
    "\n",
    "    with tqdm(total=data_path.stat().st_size) as pbar:\n",
    "        with data_path.open(encoding='utf-8') as file:\n",
    "            sample = {}\n",
    "            determine_attributes = False\n",
    "\n",
    "            i = 0\n",
    "            list_name = ''\n",
    "            while line := file.readline():\n",
    "                if LIST_IDENTIFIER in line:\n",
    "                    if list_name in file_metadata: \n",
    "                        file_metadata[list_name]['end_line'] = i-2\n",
    "                        file_metadata[list_name]['attributes'] = list(sample.keys())\n",
    "                        file_metadata[list_name]['sample'] = sample\n",
    "\n",
    "                    list_name = get_attribute_from_json_line(line)\n",
    "                    file_metadata[list_name] = {'start_line':i}\n",
    "\n",
    "                    sample = {}\n",
    "                    determine_attributes = True\n",
    "                \n",
    "                elif determine_attributes and '}' in line:\n",
    "                    determine_attributes = False\n",
    "                \n",
    "                elif determine_attributes and ':' in line:\n",
    "                    attribute, value = get_json_key_value_pair(line)\n",
    "                    sample[attribute] = value\n",
    "                    \n",
    "                i += 1\n",
    "                if not i % 1000:\n",
    "                    pbar.update(file.tell() - pbar.n)\n",
    "\n",
    "            if list_name in file_metadata: \n",
    "                file_metadata[list_name]['end_line'] = i-2\n",
    "                file_metadata[list_name]['attributes'] = list(sample.keys())\n",
    "                file_metadata[list_name]['sample'] = sample\n",
    "                \n",
    "    if save_path is not None:\n",
    "        with save_path.open('w', encoding='utf-8') as outfile:\n",
    "            json.dump(file_metadata, outfile)\n",
    "\n",
    "    return file_metadata\n",
    "\n",
    "def get_json_key_value_pair(json_line: str) -> tuple[str]:\n",
    "    attribute, value = list(\n",
    "        map(str.strip, json_line\\\n",
    "            .replace('\"','')\\\n",
    "            .replace(',\\n','')\\\n",
    "            .replace('\\n','')\\\n",
    "            .split(':',1))\n",
    "    )\n",
    "    return attribute, value\n",
    "\n",
    "def json_list_to_csv(json_file: Path,\n",
    "                     list_metadata: dict,\n",
    "                     output_file: Path,\n",
    "                     attributes: list[str] = None) -> None:\n",
    "    \n",
    "    if attributes is None:\n",
    "        attributes = list_metadata['attributes']\n",
    "\n",
    "    with tqdm(total=list_metadata['end_line']) as pbar:\n",
    "        with json_file.open(encoding='utf-8') as file:\n",
    "            with output_file.open('w', encoding='utf-8',newline='') as output_file:\n",
    "                csvwriter = csv.DictWriter(output_file, fieldnames=attributes, delimiter=\";\")\n",
    "                csvwriter.writeheader()\n",
    "\n",
    "                data_point = {}\n",
    "                for i in range(list_metadata['end_line']):\n",
    "                    line = file.readline()\n",
    "\n",
    "                    if not i % 1000:\n",
    "                        pbar.update(1000)\n",
    "\n",
    "                    if i <= list_metadata['start_line']: # Skip to list\n",
    "                        continue\n",
    "                    \n",
    "                    if ':' in line:\n",
    "                        attribute, value = get_json_key_value_pair(line)\n",
    "                        if attribute in attributes:\n",
    "                            data_point[attribute] = value\n",
    "\n",
    "                    if '}' in line and len(data_point.keys()) > 0:\n",
    "                        csvwriter.writerow(data_point)\n",
    "                        data_point = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bygnings- og Boligregistret (BBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_building_list\n",
    "#sys.path.append(os.getcwd())\n",
    "\n",
    "BUILDING_LIST_LINE_NUMBER = 55_911_629\n",
    "\n",
    "def extract_specific_sequence_from_json_file(outfile_name: str, start_index: int = 0, start_character: str = '[', end_character: str = ']') -> None:\n",
    "    i = 0\n",
    "    outfile_path = DATA_FOLDER / outfile_name\n",
    "    with tqdm(total=BBR_DATA_PATH.stat().st_size) as pbar:\n",
    "        with BBR_DATA_PATH.open(encoding='utf-8') as file:\n",
    "            with outfile_path.open('w', encoding='utf-8') as outfile:\n",
    "                while line := file.readline():\n",
    "                    i += 1\n",
    "                    if not i % 1000:\n",
    "                        pbar.update(file.tell() - pbar.n)\n",
    "                    if i < start_index:\n",
    "                        continue\n",
    "                    outfile.write(line)\n",
    "\n",
    "                    last_character_of_line = line.strip()[-1]\n",
    "                    if last_character_of_line == end_character:\n",
    "                        break\n",
    "\n",
    "\n",
    "extract_specific_sequence_from_json_file('BuildingListBBR_FULL.json', BUILDING_LIST_LINE_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_indices_of_lists\n",
    "\n",
    "#sys.path.append(os.getcwd())\n",
    "\n",
    "LIST_IDENTIFIER = 'List\":'\n",
    "\n",
    "def get_attribute_from_json_line(line: str)->str:\n",
    "    return line.split(':')[0][1:-1]\n",
    "\n",
    "def get_indices_of_lists()->dict:\n",
    "    i = 0\n",
    "    list_indices = dict()\n",
    "    with tqdm(total=BBR_DATA_PATH.stat().st_size) as pbar:\n",
    "        with BBR_DATA_PATH.open(encoding='utf-8') as file:\n",
    "            while line := file.readline():\n",
    "                if LIST_IDENTIFIER in line:\n",
    "                    attribute = get_attribute_from_json_line(line)\n",
    "                    list_indices[attribute] = i\n",
    "                    \n",
    "                i += 1\n",
    "                if not i % 1000:\n",
    "                    pbar.update(file.tell() - pbar.n)\n",
    "\n",
    "    return list_indices\n",
    "\n",
    "def save_indices_of_list(indices_of_list: dict, outfile_name: str):\n",
    "    outfile_path = DATA_FOLDER / outfile_name\n",
    "    with outfile_path.open('w', encoding='utf-8') as outfile:\n",
    "        json.dump(indices_of_list, outfile)\n",
    "\n",
    "\n",
    "indices_of_lists = get_indices_of_lists()\n",
    "save_indices_of_list(indices_of_lists, 'BBR_FULL_indices_of_lists.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_BBR\n",
    "\n",
    "# %% Jupyter extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %% Imports\n",
    "from utils.preprocessing import get_file_metadata, json_list_to_csv\n",
    "\n",
    "#%%\n",
    "FullBuildingList_path = DATA_FOLDER / 'BuildingListBBR_FULL.json'\n",
    "BuildingList_path = DATA_FOLDER / 'BBR_BuildingList.csv'\n",
    "\n",
    "#%%\n",
    "BBR_metadata_path = DATA_FOLDER / 'BBR_metadata.json'\n",
    "BBR_metadata = get_file_metadata(BBR_DATA_PATH, BBR_metadata_path)\n",
    "BBR_metadata\n",
    "\n",
    "# %% HusnummerList\n",
    "json_list_to_csv(\n",
    "    BBR_DATA_PATH,\n",
    "    BBR_metadata['BygningList'],\n",
    "    BuildingList_path,\n",
    "    attributes=[\n",
    "        'id_lokalId',\n",
    "        'status',\n",
    "        'byg007Bygningsnummer',\n",
    "        'byg021BygningensAnvendelse',\n",
    "        'byg026Opførelsesår',\n",
    "        'byg027OmTilbygningsår',\n",
    "        'byg030Vandforsyning',\n",
    "        'byg031Afløbsforhold',\n",
    "        'byg032YdervæggensMateriale',\n",
    "        'byg033Tagdækningsmateriale',\n",
    "        'byg034SupplerendeYdervæggensMateriale',\n",
    "        'byg035SupplerendeTagdækningsMateriale',\n",
    "        'byg036AsbestholdigtMateriale',\n",
    "        'byg038SamletBygningsareal',\n",
    "        'byg039BygningensSamledeBoligAreal',\n",
    "        'byg040BygningensSamledeErhvervsAreal',\n",
    "        'byg041BebyggetAreal',\n",
    "        'byg042ArealIndbyggetGarage',\n",
    "        'byg043ArealIndbyggetCarport',\n",
    "        'byg044ArealIndbyggetUdhus',\n",
    "        'byg045ArealIndbyggetUdestueEllerLign',\n",
    "        'byg046SamletArealAfLukkedeOverdækningerPåBygningen',\n",
    "        'byg047ArealAfAffaldsrumITerrænniveau',\n",
    "        'byg048AndetAreal',\n",
    "        'byg049ArealAfOverdækketAreal',\n",
    "        'byg051Adgangsareal',\n",
    "        'byg054AntalEtager',\n",
    "        'byg055AfvigendeEtager',\n",
    "        'byg056Varmeinstallation',\n",
    "        'byg057Opvarmningsmiddel',\n",
    "        'byg058SupplerendeVarme',\n",
    "        'byg069Sikringsrumpladser',\n",
    "        'byg070Fredning',\n",
    "        'byg111StormrådetsOversvømmelsesSelvrisiko',\n",
    "        'byg112DatoForRegistreringFraStormrådet',\n",
    "        'byg130ArealAfUdvendigEfterisolering',\n",
    "        'byg136PlaceringPåSøterritorie',\n",
    "        'jordstykke',\n",
    "        'husnummer',\n",
    "        'ejerlejlighed',\n",
    "        'grund'\n",
    "    ])\n",
    "\n",
    "#%%\n",
    "BygningList_data = pd.read_csv(BuildingList_path, delimiter=';')\n",
    "# %%\n",
    "columns_to_remove = []\n",
    "for col in BygningList_data.columns:\n",
    "    if not (~BygningList_data[col].isna()).any():\n",
    "        print(col)\n",
    "        columns_to_remove.append(col)\n",
    "\n",
    "new_BygningList_data = BygningList_data.loc[:, set(BygningList_data.columns) - set(columns_to_remove)]\n",
    "\n",
    "#%%\n",
    "new_BygningList_data.to_csv(\n",
    "    DATA_FOLDER / 'BBR_BuildingList2.csv', sep=';')\n",
    "\n",
    "#%% Remove Apartments\n",
    "BygningList_data_no_apartments = new_BygningList_data[new_BygningList_data['ejerlejlighed'].isna()]\n",
    "\n",
    "#%% Remove non-residence buildings\n",
    "residence_building_ids = [120, 121, 122, 130, 131, 132, 140, 190]\n",
    "BygningList_data_residence_places = BygningList_data_no_apartments[BygningList_data_no_apartments['byg021BygningensAnvendelse'].isin(residence_building_ids)]\n",
    "# %% Save the filtered version\n",
    "BygningList_data_residence_places.to_csv(\n",
    "    DATA_FOLDER / 'BBR_BuildingList_filtered.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Danmarks Adresseregister (DAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_DAR\n",
    "\n",
    "# %% Jupyter extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.preprocessing import get_file_metadata, json_list_to_csv\n",
    "\n",
    "# %% Data paths\n",
    "DAR_path = Path('D:/DAR_Aktuelt_Totaludtraek_JSON_HF_20231105180006/DAR_Aktuelt_Totaludtraek_JSON_HF_20231105180006.json')\n",
    "HusnummerList_path = Path('data/DAR/DAR_HusnummerList.csv')\n",
    "PostnummerList_path = Path('data/DAR/DAR_PostnummerList.csv')\n",
    "NavngivenVejList_path = Path('data/DAR/DAR_NavngivenVejList.csv')\n",
    "#############################################################\n",
    "# Gathering data\n",
    "#############################################################\n",
    "# %% Generate Metadata\n",
    "DAR_metadata_path = Path('data/DAR/DAR_metadata.json')\n",
    "DAR_metadata = get_file_metadata(DAR_path, DAR_metadata_path)\n",
    "DAR_metadata\n",
    "########################## DAR data to csv ##########################\n",
    "# %% HusnummerList\n",
    "json_list_to_csv(\n",
    "    DAR_path,\n",
    "    DAR_metadata['HusnummerList'],\n",
    "    HusnummerList_path,\n",
    "    attributes=[\n",
    "        'id_lokalId',\n",
    "        'status',\n",
    "        'adgangsadressebetegnelse',\n",
    "        'husnummertekst',\n",
    "        'navngivenVej',\n",
    "        'postnummer'\n",
    "    ])\n",
    "\n",
    "# %% PostnummerList\n",
    "json_list_to_csv(\n",
    "    DAR_path,\n",
    "    DAR_metadata['PostnummerList'],\n",
    "    PostnummerList_path,\n",
    "    attributes=[\n",
    "        'id_lokalId',\n",
    "        'status',\n",
    "        'navn',\n",
    "        'postnr'\n",
    "    ])\n",
    "# %% NavngivenVejList\n",
    "json_list_to_csv(\n",
    "    DAR_path,\n",
    "    DAR_metadata['NavngivenVejList'],\n",
    "    NavngivenVejList_path,\n",
    "    attributes=[\n",
    "        'id_lokalId',\n",
    "        'status',\n",
    "        'vejnavn'\n",
    "    ])\n",
    "\n",
    "########################## Join DAR files ##########################\n",
    "# %% Loading DAR data\n",
    "HusnummerList_data = pd.read_csv(HusnummerList_path, delimiter=';')\\\n",
    "                        .rename(columns={\n",
    "                            'id_lokalId':'husnummer_id',\n",
    "                            'adgangsadressebetegnelse': 'address',\n",
    "                            'husnummertekst':'house_nr',\n",
    "                            'navngivenVej':'street_id',\n",
    "                            'postnummer':'postal_id'\n",
    "                        })\n",
    "NavngivenVejList_data = pd.read_csv(NavngivenVejList_path, delimiter=';')\\\n",
    "                        .rename(columns={\n",
    "                            'id_lokalId':'street_id',\n",
    "                            'vejnavn':'street_name'\n",
    "                        })\n",
    "PostnummerList_data = pd.read_csv(PostnummerList_path, delimiter=';')\\\n",
    "                        .rename(columns={\n",
    "                            'id_lokalId':'postal_id',\n",
    "                            'navn': 'city_name',\n",
    "                            'postnr':'postal_nr'\n",
    "                        })\n",
    "# %% Filter addresses\n",
    "has_street = ~pd.isna(HusnummerList_data['street_id'])\n",
    "is_house = HusnummerList_data['house_nr'].apply(lambda x: str.isnumeric(str(x)))\n",
    "\n",
    "DAR_data = HusnummerList_data.loc[has_street]\n",
    "DAR_data\n",
    "# %% Join DAR data\n",
    "DAR_joined_data = DAR_data.merge(NavngivenVejList_data.loc[:,['street_id','street_name']], how='left', on='street_id')\\\n",
    "                    .merge(PostnummerList_data.loc[:, ['postal_id','city_name','postal_nr']], how='left', on='postal_id')\\\n",
    "                    .loc[:, ['husnummer_id','address','house_nr','street_name','city_name','postal_nr']]\n",
    "DAR_joined_data\n",
    "# %% Save DAR data\n",
    "DAR_joined_data_path = Path('data/DAR/DAR_joined_full.csv')\n",
    "DAR_joined_data.to_csv(DAR_joined_data_path, sep=';', index=False)\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejendomsvurderinger (VUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VUR_merge. Run from command line as independent script\n",
    "    \n",
    "\"\"\"\n",
    "CL args:\n",
    "    VUR_DIR (str): Directory of VUR files e.g \"/home/downloads\". Defaults to current working directory\n",
    "\"\"\"\n",
    "if sys.argv[1]:\n",
    "    VUR_DIR = str(sys.argv[1])\n",
    "else:\n",
    "    VUR_DIR = \".\"\n",
    "\n",
    "subset = ['id']\n",
    "\n",
    "joined_files = os.path.join(VUR_DIR, \"VUR_*.csv\") \n",
    "joined_list = glob.glob(joined_files)\n",
    "df = pd.concat(map(lambda file: pd.read_csv(file, sep=';'), joined_list), ignore_index=True)\n",
    "print(df.head())\n",
    "before = len(df)\n",
    "mask = df.duplicated(subset=subset)\n",
    "\n",
    "print(f\"Before removing duplicates we have {before} rows\")\n",
    "\n",
    "df.drop_duplicates(subset=subset, inplace=True, ignore_index=True)\n",
    "df.to_csv(os.path.join(VUR_DIR, \"VUR_joined_with_nulls.csv\"), index=False, sep=';')\n",
    "\n",
    "print(f\"After removing duplicates we have {len(df)} rows\")\n",
    "print(f\"Removed {before - len(df)} rows in total\") \n",
    "df.loc[mask].to_csv(os.path.join(VUR_DIR, \"VUR_joined_duplicates.csv\"), index=False, sep=';')\n",
    "\n",
    "df_no_nulls = df.dropna(how='any')\n",
    "df_no_nulls.to_csv(os.path.join(VUR_DIR, \"VUR_joined.csv\"), index=False, sep=';')\n",
    "print(\"Dropped nulls df:\", len(df_no_nulls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribute overview\n",
    "{\n",
    "    \"id\": GUID # aka. adgangsAdresseID(VUR), husnummer_id(BBR,DAR),\n",
    "    \"postal_nr\": str # DAR,\n",
    "    \"address\": str # DAR,\n",
    "    \"evaluationYear\": int # VUR,\n",
    "    \"propertyValue\": float # VUR,\n",
    "    \"groundValue\": float # VUR, \n",
    "    \"propertyTax\": float # VUR,\n",
    "    \"groundTax\": float # VUR,\n",
    "    \"totalAddressTax\": float # VUR,\n",
    "    \"buildingUsageType\": one hot encoded # aka. byg021BygningensAnvendelse(BBR),\n",
    "    \"constructionYear\": int # aka. byg026Opførelsesår(BBR),\n",
    "    \"reconstructionOrExtensionYear\": int # byg027OmTilbygningsår(BBR),\n",
    "    \"waterSupplyType\": one hot encoded # aka. byg030Vandforsyning(BBR),\n",
    "    \"drainageType\": one hot encoded # aka. byg031Afløbsforhold(BBR),\n",
    "    \"outerWallMaterialType\": one hot encoded # aka. byg032YdervæggensMateriale(BBR),\n",
    "    \"roofMaterialType\": one hot encoded # aka. byg033Tagdækningsmateriale(BBR),\n",
    "    \"supplementOuterWallMaterialType\": one hot encoded # aka. byg034SupplerendeYdervæggensMateriale(BBR) - code is YdervæggensMateriale,\n",
    "    \"supplementRoofMaterialType\": one hot encoded # aka. byg035SupplerendeTagdækningsMateriale(BBR) - code is Tagdækningsmateriale,\n",
    "    \"asbestosHoldingMaterialType\": one hot encoded # aka. byg036AsbestholdigtMateriale(BBR),\n",
    "    \"totalBuildingArea\": float # aka. byg038SamletBygningsareal(BBR),\n",
    "    \"totalResidenceArea\": float # aka. byg039BygningensSamledeBoligAreal(BBR),\n",
    "    \"totalIndustrialArea\": float # aka. byg040BygningensSamledeErhvervsAreal(BBR),\n",
    "    \"builtArea\": float # aka. byg041BebyggetAreal (BBR),\n",
    "    \"areaOfBuiltInGarage\": float # aka. byg042ArealIndbyggetGarage (BBR),\n",
    "    \"areaOfBuiltInCarport\": float # aka. byg043ArealIndbyggetCarport (BBR),\n",
    "    \"areaOfBuiltInShed\": float # aka. byg044ArealIndbyggetUdhus (BBR),\n",
    "    \"areaOfBuiltInConservatoryOrSimilar\": float # aka. byg045ArealIndbyggetUdestueEllerLign (BBR),\n",
    "    \"totalAreaOfClosedCoveringsOnTheBuilding\": float # aka. byg046SamletArealAfLukkedeOverdækningerPåBygningen (BBR),\n",
    "    \"areaOfWasteRoomAtGroundLevel\": float # aka. byg047ArealAfAffaldsrumITerrænniveau (BBR),\n",
    "    \"otherArea\": float # aka. byg048AndetAreal (BBR),\n",
    "    \"areaOfCoveredArea\": float # aka. byg049ArealAfOverdækketAreal (BBR),\n",
    "    \"accessArea\": float # aka. byg051Adgangsareal (BBR),\n",
    "    \"numberOfFloors\": int # aka. byg054AntalEtager (BBR),\n",
    "    \"deviantFloors\": one hot encoded # aka. byg055AfvigendeEtager (BBR),\n",
    "    \"heatingInstallation\": one hot encoded # aka. byg056Varmeinstallation (BBR),\n",
    "    \"heatingMedium\": one hot encoded # aka. byg057Opvarmningsmiddel (BBR),\n",
    "    \"supplementaryHeating\": one hot encoded # aka. byg058SupplerendeVarme (BBR) - code list is Opvarmningsmiddel,\n",
    "    \"shelterSpaces\": int # aka. byg069Sikringsrumpladser (BBR),\n",
    "    \"preservation\": one hot encoded # aka. byg070Fredning (BBR),\n",
    "    \"stormCouncilsFloodSelfRisk\": one hot encoded # aka. byg111StormrådetsOversvømmelsesSelvrisiko (BBR) - code list is OversvømmelsesSelvrisiko,\n",
    "    \"areaOfExternalInsulation\": float # aka. byg130ArealAfUdvendigEfterisolering (BBR),\n",
    "    \"locationOnLakeTerritory\": bool # aka. byg136PlaceringPåSøterritorie (BBR),\n",
    "    \"totalAddressValue\": float # sum of propertyValue and groundValue,\n",
    "    \"region\": int # first digit of postal_nr\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# %% Settings\n",
    "data_path = Path('data')\n",
    "BBR_path = data_path / Path('BBR/BBR_BuildingList.csv')\n",
    "VUR_path = data_path / Path('VUR/VUR_joined.csv')\n",
    "DAR_path = data_path / Path('DAR/DAR_joined.csv')\n",
    "\n",
    "# %% Load data\n",
    "BBR = pd.read_csv(BBR_path, delimiter=';', index_col=0)\n",
    "VUR = pd.read_csv(VUR_path, delimiter=';')\\\n",
    "    .drop(columns=['id'])\\\n",
    "    .rename(columns={'vurderingsaar':'evaluationYear', 'adgangsAdresseID':'id'})\n",
    "DAR = pd.read_csv(DAR_path, delimiter=';')\\\n",
    "    .rename(columns={'husnummer_id':'id'})\n",
    "# %% Rename BBR columns\n",
    "DA_EN_BBR_column_map = {\n",
    "    \"byg021BygningensAnvendelse\" : \"buildingUsageType\",\n",
    "     \"byg026Opførelsesår\" : \"constructionYear\",\n",
    "     \"byg027OmTilbygningsår\" : \"reconstructionOrExtensionYear\",\n",
    "     \"byg030Vandforsyning\" : \"waterSupplyType\",\n",
    "     \"byg031Afløbsforhold\" : \"drainageType\",\n",
    "     \"byg032YdervæggensMateriale\" : \"outerWallMaterialType\",\n",
    "     \"byg033Tagdækningsmateriale\" : \"roofMaterialType\",\n",
    "     \"byg034SupplerendeYdervæggensMateriale\" : \"supplementOuterWallMaterialType\",\n",
    "     \"byg035SupplerendeTagdækningsMateriale\" : \"supplementRoofMaterialType\",\n",
    "     \"byg036AsbestholdigtMateriale\" : \"asbestosHoldingMaterialType\",\n",
    "     \"byg038SamletBygningsareal\" : \"totalBuildingArea\",\n",
    "     \"byg039BygningensSamledeBoligAreal\" : \"totalResidenceArea\",\n",
    "     \"byg040BygningensSamledeErhvervsAreal\" : \"totalIndustrialArea\",\n",
    "     \"byg041BebyggetAreal\"  : \"builtArea\",\n",
    "     \"byg042ArealIndbyggetGarage\"  : \"areaOfBuiltInGarage\",\n",
    "     \"byg043ArealIndbyggetCarport\"  : \"areaOfBuiltInCarport\",\n",
    "     \"byg044ArealIndbyggetUdhus\"  : \"areaOfBuiltInShed\",\n",
    "     \"byg045ArealIndbyggetUdestueEllerLign\"  : \"areaOfBuiltInConservatoryOrSimilar\",\n",
    "     \"byg046SamletArealAfLukkedeOverdækningerPåBygningen\"  : \"totalAreaOfClosedCoveringsOnTheBuilding\",\n",
    "     \"byg047ArealAfAffaldsrumITerrænniveau\"  : \"areaOfWasteRoomAtGroundLevel\",\n",
    "     \"byg048AndetAreal\"  : \"otherArea\",\n",
    "     \"byg049ArealAfOverdækketAreal\"  : \"areaOfCoveredArea\",\n",
    "     \"byg051Adgangsareal\"  : \"accessArea\",\n",
    "     \"byg054AntalEtager\"  : \"numberOfFloors\",\n",
    "     \"byg055AfvigendeEtager\"  : \"deviantFloors\",\n",
    "     \"byg056Varmeinstallation\"  : \"heatingInstallation\",\n",
    "     \"byg057Opvarmningsmiddel\"  : \"heatingMedium\",\n",
    "     \"byg058SupplerendeVarme\" : \"supplementaryHeating\",\n",
    "     \"byg069Sikringsrumpladser\"  : \"shelterSpaces\",\n",
    "     \"byg070Fredning\"  : \"preservation\",\n",
    "     \"byg111StormrådetsOversvømmelsesSelvrisiko\": \"stormCouncilsFloodSelfRisk\",\n",
    "     \"byg130ArealAfUdvendigEfterisolering\"  : \"areaOfExternalInsulation\",\n",
    "    \"byg136PlaceringPåSøterritorie\"  :     \"locationOnLakeTerritory\",\n",
    "    \"husnummer\": \"id\"\n",
    "}\n",
    "BBR.rename(columns=DA_EN_BBR_column_map, inplace=True)\n",
    "BBR.drop(columns=set(BBR.columns) - set(DA_EN_BBR_column_map.values()), inplace=True)\n",
    "BBR.columns\n",
    "# %% Get the latest BBR data for a given address\n",
    "BBR_filtered = BBR.groupby('id').max('constructionYear')\n",
    "# %% Merge data\n",
    "ds = VUR.merge(DAR, how='left', on='id')\\\n",
    "        .merge(BBR_filtered, how='inner', on='id')\n",
    "\n",
    "ds\n",
    "# %% Save dataset\n",
    "ds.to_csv('data/DS.csv',sep=';', index=False)\n",
    "\n",
    "# %% One hot encode categorical attributes\n",
    "categorical_columns = [\n",
    "    'buildingUsageType',\n",
    "    'waterSupplyType',\n",
    "    'drainageType',\n",
    "    'outerWallMaterialType',\n",
    "    'roofMaterialType',\n",
    "    'supplementOuterWallMaterialType',\n",
    "    'supplementRoofMaterialType',\n",
    "    'asbestosHoldingMaterialType',\n",
    "    'deviantFloors',\n",
    "    'heatingInstallation',\n",
    "    'heatingMedium',\n",
    "    'supplementaryHeating',\n",
    "    'preservation',\n",
    "    'stormCouncilsFloodSelfRisk'\n",
    "]\n",
    "one_hot_ds = pd.get_dummies(ds, columns=categorical_columns, dtype=int)\n",
    "# %% Save one hot ds\n",
    "one_hot_ds.to_csv('data/DS_with_one_hot.csv',sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans_hyperparameter\n",
    "# %% Load data\n",
    "ds_path = 'data/DS_RAndD_minmax.csv'\n",
    "print('[Loading data] Start:', ds_path)\n",
    "ds = pd.read_csv(ds_path, index_col='id')\n",
    "print('[Loading data] Done')\n",
    "# %% Separate data\n",
    "feature_names = ds.columns[ds.columns != PREDICTION_VAR]\n",
    "feature_data = ds[feature_names]\n",
    "\n",
    "# %% Determine optimal K\n",
    "sum_of_sq_dists = []\n",
    "Ks = range(100,300,25)\n",
    "for K in Ks:\n",
    "    kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "    kmeans.fit(feature_data)\n",
    "    sum_of_sq_dists.append(kmeans.inertia_)\n",
    "plt.plot(Ks,sum_of_sq_dists, '.-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Optimal K')\n",
    "plt.savefig('../assets/Kmeans.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmeans_clustering\n",
    "# %% Load data\n",
    "print('[Loading data] Start')\n",
    "ds = pd.read_csv('data/DS_RAndD_minmax.csv', index_col='id')\n",
    "print('[Loading data] Done')\n",
    "# %% Separate data\n",
    "feature_names = ds.columns[ds.columns != PREDICTION_VAR]\n",
    "feature_data = ds[feature_names]\n",
    "# %% Hyperparameters\n",
    "K = 250\n",
    "\n",
    "# %% Fit clusters\n",
    "print('[Determining clusters] Start')\n",
    "kmeans = KMeans(n_clusters=K)\n",
    "labels = kmeans.fit_predict(feature_data)\n",
    "print('[Determining clusters] Done')\n",
    "# %% Save clusters\n",
    "np.save(f\"data/clusterings/Kmeans_clusterings_K{str(K)}.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN_hyperparameter_analysis\n",
    "# %% Load data\n",
    "ds_path = 'data/DS_reduced_and_dropped.csv'\n",
    "# ds_path = 'data/DS_RAndD_minmax.csv'\n",
    "# ds_path = 'data/DS_RAndD_normalized.csv'\n",
    "print('[Loading data] Start:', ds_path)\n",
    "ds = pd.read_csv(ds_path, index_col='id')\n",
    "print('[Loading data] Done')\n",
    "# %% Separate data\n",
    "feature_names = ds.columns[ds.columns != PREDICTION_VAR]\n",
    "feature_data = ds[feature_names]\n",
    "\n",
    "# %% Determine distances\n",
    "NN_model = NearestNeighbors(n_neighbors=2, n_jobs=-1).fit(feature_data)\n",
    "distances, indices = NN_model.kneighbors(feature_data)\n",
    "\n",
    "# %% Save distances\n",
    "np.save('data/distances/DBSCAN_HP_distances.npy',distances)\n",
    "# np.save('data/distances/DBSCAN_HP_distances_minmax.npy',distances)\n",
    "# np.save('data/distances/DBSCAN_HP_distances_normalized.npy',distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN_clustering\n",
    "# %% Load data\n",
    "print('[Loading data] Start')\n",
    "ds = pd.read_csv('data/DS_RAndD_minmax.csv', index_col='id')\n",
    "print('[Loading data] Done')\n",
    "# %% Separate data\n",
    "feature_names = ds.columns[ds.columns != PREDICTION_VAR]\n",
    "feature_data = ds[feature_names]\n",
    "# %% Hyperparameters\n",
    "# https://medium.com/@tarammullin/dbscan-parameter-estimation-ff8330e3a3bd\n",
    "eps = 1.1\n",
    "min_samples = 2*len(feature_names)\n",
    "\n",
    "# %% Fit clusters\n",
    "print('[Determining clusters] Start')\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
    "dbscan.fit(feature_data)\n",
    "print('[Determining clusters] Done')\n",
    "# %% Save clusters\n",
    "db_clusterings = dbscan.labels_\n",
    "np.save(f\"data/clusterings/DBSCAN_clusterings_minmax_eps{str(eps).replace('.','_')}_ms{min_samples}.npy\", db_clusterings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "was run on HPC using ```queue_run.sh```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/sh\n",
    "#BSUB -q hpc\n",
    "#BSUB -J DBSCAN\n",
    "#BSUB -n 4\n",
    "#BSUB -W 3:00\n",
    "#BSUB -R \"rusage[mem=64GB]\"\n",
    "#BSUB -o out/JLOG_%J.out\n",
    "#BSUB -N \n",
    "\n",
    "echo '=================== Load modules: Started ==================='\n",
    "module load python3\n",
    "#module load scipy/1.10.1-python-3.11.3\n",
    "#nvidia-smi\n",
    "#module load cuda/12.1\n",
    "echo '=================== Load modules: Succeded ==================='\n",
    "\n",
    "echo '=================== Activate environment: Start ==================='\n",
    "source 02807/bin/activate\n",
    "echo '=================== Activate environment: Succeded ==================='\n",
    "\n",
    "echo '=================== Executing script: Start ==================='\n",
    "python3 analysis/DBSCAN_clustering.py\n",
    "echo '=================== Executing script: Succeded ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Polynomial regression)\n",
    "We found out this did not work too well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load data\n",
    "print('[Loading data] Start')\n",
    "ds = pd.read_csv('data/DS_reduced_and_dropped.csv', index_col='id')\n",
    "print('[Loading data] Done')\n",
    "# %% Separate data\n",
    "# Prediction variable\n",
    "prediction_property = 'totalAddressValue'\n",
    "total_address_values = ds[prediction_property]\n",
    "\n",
    "# Feature data\n",
    "feature_names = ds.columns[ds.columns != prediction_property]\n",
    "feature_data = ds[feature_names]\n",
    "#############################################################\n",
    "# %% No scaling\n",
    "#############################################################\n",
    "print('[No scaling] Fitting')\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\\\n",
    "    .fit_transform(feature_data)\n",
    "poly_model = LinearRegression(n_jobs=-1).fit(poly_features, total_address_values)\n",
    "print('[No scaling] Fitting done')\n",
    "\n",
    "# Coefficient of determination\n",
    "coef_det = poly_model.score(poly_features, total_address_values)\n",
    "print(f'[No scaling] Coefficient of determination: {coef_det}')\n",
    "\n",
    "if coef_det > 0.315:\n",
    "    with open('data/models/PR_model.sav', 'wb') as f:\n",
    "        pickle.dump(poly_model, f)\n",
    "#############################################################\n",
    "# %% Standard scaling\n",
    "#############################################################\n",
    "scaler = StandardScaler()\n",
    "normalized_feature_data = scaler.fit_transform(feature_data)\n",
    "\n",
    "print('[Standard scaling] Fitting')\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\\\n",
    "    .fit_transform(normalized_feature_data)\n",
    "poly_model = LinearRegression(n_jobs=-1).fit(poly_features, total_address_values)\n",
    "print('[Standard scaling] Fitting done')\n",
    "\n",
    "# Coefficient of determination\n",
    "coef_det = poly_model.score(poly_features, total_address_values)\n",
    "print(f'[Standard scaling] Coefficient of determination: {coef_det}')\n",
    "\n",
    "if coef_det > 0.315:\n",
    "    with open('data/models/PR_standard_model.sav', 'wb') as f:\n",
    "        pickle.dump(poly_model, f)\n",
    "#############################################################\n",
    "# %% MinMax scaling\n",
    "#############################################################\n",
    "scaler = MinMaxScaler()\n",
    "normalized_feature_data = scaler.fit_transform(feature_data)\n",
    "\n",
    "print('[MinMax scaling] Fitting')\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\\\n",
    "    .fit_transform(normalized_feature_data)\n",
    "poly_model = LinearRegression(n_jobs=-1).fit(poly_features, total_address_values)\n",
    "print('[MinMax scaling] Fitting done')\n",
    "\n",
    "# Coefficient of determination\n",
    "coef_det = poly_model.score(poly_features, total_address_values)\n",
    "print(f'[MinMax scaling] Coefficient of determination: {coef_det}')\n",
    "\n",
    "if coef_det > 0.315:\n",
    "    with open('data/models/PR_minmax_model.sav', 'wb') as f:\n",
    "        pickle.dump(poly_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abnormality detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# %% Load data\n",
    "sys.stdout.write('[Loading data] Start\\n')\n",
    "ds = pd.read_csv('data/DS_RAndD_minmax.csv', index_col='id')\n",
    "\n",
    "evaluations = ds[PREDICTION_VAR]\n",
    "feature_names = ds.columns[ds.columns != PREDICTION_VAR]\n",
    "feature_data = ds[feature_names]\n",
    "sys.stdout.write('[Loading data] Done\\n')\n",
    "# %% Load clusters\n",
    "sys.stdout.write('[Loading clusterings] Start\\n')\n",
    "clusterings = np.load('data/clusterings/Kmeans_clusterings_K250.npy')\n",
    "sys.stdout.write('[Loading clusterings] Done\\n')\n",
    "# %% Hyperparameters\n",
    "K = 100\n",
    "T = 1/100\n",
    "reference_dist = norm()\n",
    "# %% Abnormality analysis\n",
    "def check_outlier_abnormality(row, NN_model, evaluations):\n",
    "    distances, indicies = NN_model.kneighbors(row[feature_names].to_numpy().reshape(1, -1))\n",
    "    NN_evaluations = evaluations[indicies[0][1:]]\n",
    "    n = len(NN_evaluations)\n",
    "    tobs = (row[PREDICTION_VAR] - NN_evaluations.mean())/(NN_evaluations.std()/np.sqrt(n))\n",
    "    p = 2*(1 - reference_dist.cdf(abs(tobs)))\n",
    "\n",
    "    return 0 if p > T else 1\n",
    "\n",
    "def check_abnormality(row, evaluations):\n",
    "    evaluations = evaluations[evaluations.index != row.name]\n",
    "    tobs = (row[PREDICTION_VAR] - evaluations.mean())/(evaluations.std())\n",
    "    p = (1 - reference_dist.cdf(abs(tobs)))\n",
    "\n",
    "    return 0 if p > T else 1\n",
    "\n",
    "abnormality_ds = pd.DataFrame()\n",
    "cluster_ids = np.unique(clusterings)\n",
    "for cluster_id in tqdm(cluster_ids):\n",
    "    cluster_mask = clusterings==cluster_id\n",
    "    cluster = ds[cluster_mask]\n",
    "    cluster['cluster'] = cluster_id\n",
    "\n",
    "    if cluster_id == -1: # Outliers\n",
    "        cluster['isAbnormal'] = 1\n",
    "        # cluster_NN = NearestNeighbors(n_neighbors=K+1).fit(feature_data) # K+1 because it will find itself\n",
    "        # cluster['isAbnormal'] = cluster.progress_apply(lambda row: check_outlier_abnormality(row, cluster_NN, evaluations), axis=1)\n",
    "    else:\n",
    "        cluster['isAbnormal'] = cluster.progress_apply(lambda row: check_abnormality(row, cluster[PREDICTION_VAR]), axis=1)\n",
    "\n",
    "    abnormality_ds = pd.concat([abnormality_ds, cluster])\n",
    "# %% Save abnormality\n",
    "abnormality_ds.to_csv('data/DS_abnormality.csv')\n",
    "# %%\n",
    "stds = []\n",
    "for cluster_id in tqdm(cluster_ids):\n",
    "    cluster_mask = clusterings==cluster_id\n",
    "    cluster = ds[cluster_mask]\n",
    "    \n",
    "    stds.append(cluster[PREDICTION_VAR].std())\n",
    "\n",
    "stds\n",
    "# %% Check abnormal valuations\n",
    "abnormalities = abnormality_ds.loc[(abnormality_ds['isAbnormal'] == 1) & (abnormality_ds['cluster'] != -1)]\n",
    "DS = pd.read_csv('/data/DS.csv', sep=';', index_col='id')\n",
    "DS_sub = DS[list(set(DS.columns) - set(abnormalities.columns))]\n",
    "# %%\n",
    "abnorm_ds = abnormalities.merge(DS_sub, how='inner', on='id')\n",
    "abnorm_ds[['address','propertyValue','groundValue']]\n",
    "# %% Normal\n",
    "normal = abnormality_ds.loc[abnormality_ds['isAbnormal'] == 0]\n",
    "normal_ds = normal.merge(DS_sub, how='inner', on='id')\n",
    "normal_ds[['address','propertyValue','groundValue']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# %% Load data\n",
    "print('[Loading data] Start')\n",
    "ds = pd.read_csv('../data/DS_abnormality.csv', index_col='id')\n",
    "print('[Loading data] Done')\n",
    "# %% Configuration\n",
    "feature_names = ds.columns[ds.columns != PREDICTION_VAR]\n",
    "seed = 42\n",
    "T = 1/100\n",
    "reference_dist = norm()\n",
    "multipliers = [1/2,2]\n",
    "N = 1000\n",
    "K = 5\n",
    "############################################################\n",
    "# Creating test dataset\n",
    "############################################################\n",
    "# %% Get normal obs and change their evaluation\n",
    "normal_obs = ds.loc[ds['isAbnormal'] == 0].sample(n=N//2, random_state=seed).copy()\n",
    "normal_obs['multiplier'] = np.random.choice(multipliers, len(normal_obs), replace=True)\n",
    "normal_obs[PREDICTION_VAR] = normal_obs[PREDICTION_VAR]*normal_obs['multiplier']\n",
    "normal_obs['label'] = 1\n",
    "normal_obs\n",
    "\n",
    "# %% Get abnormal ones and change evaluation to the average of the K nearest normal ones to check if it will make it normal\n",
    "def get_average_of_normal_KNN(abnormal_row, NN_model: NearestNeighbors, evaluations: pd.Series):\n",
    "    distances, indicies = NN_model.kneighbors(abnormal_row[feature_names].to_numpy().reshape(1, -1))\n",
    "    NN_evaluations = evaluations[indicies[0][1:]]\n",
    "    \n",
    "    return NN_evaluations.mean()\n",
    "\n",
    "abnormal_obs = ds.loc[(ds['isAbnormal'] == 1) & (ds['cluster'] != -1)].sample(n=N//2, random_state=seed).copy()\n",
    "for cluster_id in tqdm(abnormal_obs['cluster'].unique()):\n",
    "    norm_cluster = ds.loc[(ds['cluster'] == cluster_id) & (ds['isAbnormal'] == 0)]\n",
    "    abnorm_mask = abnormal_obs['cluster'] == cluster_id\n",
    "    abnorm_cluster = abnormal_obs.loc[abnorm_mask]\n",
    "\n",
    "    cluster_NN = NearestNeighbors(n_neighbors=K).fit(norm_cluster[feature_names])\n",
    "    abnormal_obs.loc[abnorm_mask, PREDICTION_VAR] = abnorm_cluster.apply(lambda row: get_average_of_normal_KNN(row, cluster_NN, norm_cluster[PREDICTION_VAR]), axis=1)\n",
    "\n",
    "abnormal_obs['label'] = 0\n",
    "abnormal_obs\n",
    "# %% Test set\n",
    "ds_test = pd.concat([normal_obs, abnormal_obs])\n",
    "ds_test\n",
    "\n",
    "############################################################\n",
    "# Creating test dataset\n",
    "############################################################\n",
    "# %% Predict\n",
    "for cluster_id in tqdm(ds_test['cluster'].unique()):\n",
    "    cluster = ds.loc[ds['cluster'] == cluster_id]\n",
    "    cluster_evals = cluster[PREDICTION_VAR]\n",
    "\n",
    "    test_cluster_mask = ds_test['cluster'] == cluster_id\n",
    "    test_cluster = ds_test.loc[test_cluster_mask]\n",
    "    zs = (test_cluster[PREDICTION_VAR] - cluster_evals.mean())/(cluster_evals.std())\n",
    "    ps = (1 - reference_dist.cdf(np.abs(zs)))\n",
    "\n",
    "    ds_test.loc[test_cluster_mask,'prediction'] = (ps<T).astype(int)\n",
    "\n",
    "ds_test\n",
    "\n",
    "# %% Evaluation\n",
    "print(confusion_matrix(ds_test['label'], ds_test['prediction'], normalize='true'))\n",
    "(ds_test['label'] == ds_test['prediction']).mean()\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
